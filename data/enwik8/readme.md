
## enwik8 dataset

after running `prepare.py` (preprocess) we get:

- train.bin is ~48MB, dev.bin ~3MB, test.bin ~3MB
- train has ~90B tokens (90,000,000,000)
- dev has ~5B tokens (5,000,000,000)
- test has ~5B tokens (5,000,000,000)
this came from 1,015,221 documents in total.

references:
- enwik8 dataset is a widely used benchmark for text compression algorithms.
- [enwik8 dataset](https://)